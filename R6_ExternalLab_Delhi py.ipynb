{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YYk8NG3yOIT9"
   },
   "source": [
    "### A MNIST-like fashion product database\n",
    "\n",
    "In this, we classify the images into respective classes given in the dataset. We use a Neural Net and a Deep Neural Net in Keras to solve this and check the accuracy scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tFO6PuxzOIT_"
   },
   "source": [
    "### Load tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 65
    },
    "colab_type": "code",
    "id": "efNjNImfOIUC",
    "outputId": "7f7653da-4579-4cfd-fe6a-a5b840e448e7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p style=\"color: red;\">\n",
       "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
       "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
       "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
       "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "l9C4aAIGOIUH",
    "outputId": "043ef04b-7c79-4666-8adc-11f8bfe31967"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.15.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HcoZBStrOIUQ"
   },
   "source": [
    "### Collect Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "XA1WsFSeOIUS",
    "outputId": "e3774868-b4f2-4c97-ab4e-12d5afe6cdc2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aIFHE8Jmf4m-"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 217
    },
    "colab_type": "code",
    "id": "qnbx7TyQOIUY",
    "outputId": "7f0c309e-cf73-4251-9a50-2e2c8a7b4b91"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "32768/29515 [=================================] - 0s 3us/step\n",
      "40960/29515 [=========================================] - 0s 2us/step\n",
      "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "26427392/26421880 [==============================] - 2s 0us/step\n",
      "26435584/26421880 [==============================] - 2s 0us/step\n",
      "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "16384/5148 [===============================================================================================] - 0s 0us/step\n",
      "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "4423680/4422102 [==============================] - 1s 0us/step\n",
      "4431872/4422102 [==============================] - 1s 0us/step\n"
     ]
    }
   ],
   "source": [
    "(trainX, trainY), (testX, testY) = keras.datasets.fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "UbiHj5YPOIUc",
    "outputId": "91e2758f-4bdc-409f-d053-cc1f8829845d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9 2 1 1 6]\n"
     ]
    }
   ],
   "source": [
    "print(testY[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lDAYzkwyOIUj"
   },
   "source": [
    "### Convert both training and testing labels into one-hot vectors.\n",
    "\n",
    "**Hint:** check **tf.keras.utils.to_categorical()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vBlfYlANOIUk"
   },
   "outputs": [],
   "source": [
    "trainY = tf.keras.utils.to_categorical(trainY, num_classes=10)\n",
    "testY = tf.keras.utils.to_categorical(testY, num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 126
    },
    "colab_type": "code",
    "id": "RHV3b9mzOIUq",
    "outputId": "00b41c26-97fc-455e-816d-a1003111b449",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 10)\n",
      "('First 5 examples now are: ', array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "print(trainY.shape)\n",
    "print('First 5 examples now are: ', trainY[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FwhQ8e7VOIUw"
   },
   "source": [
    "### Visualize the data\n",
    "\n",
    "Plot first 10 images in the triaining set and their labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AvDML2OoOIUx",
    "outputId": "9dafc94e-61a8-4089-be03-d143163d68aa"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlMAAABSCAYAAABwglFkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRVEURVF88P8A0wyYl+ZpGWEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f60784a5950>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label for each of the above image:\n",
      "9 0 0 3 0 2 7 2 5 5\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(trainX[0],cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l4TbJGeSOIU4"
   },
   "source": [
    "### Build a neural Network with a cross entropy loss function and sgd optimizer in Keras. The output layer with 10 neurons as we have 10 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ac06XZZTOIU6"
   },
   "outputs": [],
   "source": [
    "#Initialize Sequential model\n",
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "#Reshape data from 2D to 1D -> 28x28 to 784\n",
    "model.add(tf.keras.layers.Reshape((784,),input_shape=(28,28,)))\n",
    "\n",
    "\n",
    "\n",
    "#Add Dense Layer which provides 10 Outputs after applying softmax\n",
    "model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
    "\n",
    "#Comile the model\n",
    "model.compile(optimizer='sgd', loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3hQpLv3aOIU_"
   },
   "source": [
    "### Execute the model using model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "O59C_-IgOIVB",
    "outputId": "391588c1-9ec3-4b5b-9d8a-a426ebfc2252"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "60000/60000 [==============================] - 1s 9us/sample - loss: 177.7088 - acc: 0.2171 - val_loss: 7429.2183 - val_acc: 0.2135\n",
      "Epoch 2/100\n",
      "60000/60000 [==============================] - 0s 5us/sample - loss: 7363.5796 - acc: 0.2150 - val_loss: 11417.7998 - val_acc: 0.2551\n",
      "Epoch 3/100\n",
      "60000/60000 [==============================] - 0s 5us/sample - loss: 11419.2373 - acc: 0.2562 - val_loss: 13729.1602 - val_acc: 0.2338\n",
      "Epoch 4/100\n",
      "60000/60000 [==============================] - 0s 5us/sample - loss: 13728.7910 - acc: 0.2332 - val_loss: 13470.6484 - val_acc: 0.3687\n",
      "Epoch 5/100\n",
      "60000/60000 [==============================] - 0s 5us/sample - loss: 13473.9775 - acc: 0.3729 - val_loss: 12827.3408 - val_acc: 0.2844\n",
      "Epoch 6/100\n",
      "60000/60000 [==============================] - 0s 5us/sample - loss: 12787.4326 - acc: 0.2851 - val_loss: 11575.5400 - val_acc: 0.3752\n",
      "Epoch 7/100\n",
      "60000/60000 [==============================] - 0s 5us/sample - loss: 11483.2793 - acc: 0.3786 - val_loss: 9531.5957 - val_acc: 0.3880\n",
      "Epoch 8/100\n",
      "60000/60000 [==============================] - 0s 5us/sample - loss: 9433.0967 - acc: 0.3916 - val_loss: 8518.5039 - val_acc: 0.3022\n",
      "Epoch 9/100\n",
      "60000/60000 [==============================] - 0s 5us/sample - loss: 8439.9912 - acc: 0.3026 - val_loss: 6652.9927 - val_acc: 0.3506\n",
      "Epoch 10/100\n",
      "60000/60000 [==============================] - 0s 5us/sample - loss: 6611.5254 - acc: 0.3576 - val_loss: 4813.4790 - val_acc: 0.3816\n",
      "Epoch 11/100\n",
      "60000/60000 [==============================] - 0s 5us/sample - loss: 4782.3223 - acc: 0.3797 - val_loss: 4266.8809 - val_acc: 0.4031\n",
      "Epoch 12/100\n",
      "60000/60000 [==============================] - 0s 5us/sample - loss: 4176.2803 - acc: 0.4101 - val_loss: 6089.9990 - val_acc: 0.2593\n",
      "Epoch 13/100\n",
      "60000/60000 [==============================] - 0s 4us/sample - loss: 6002.7007 - acc: 0.2608 - val_loss: 9130.7930 - val_acc: 0.4096\n",
      "Epoch 14/100\n",
      "60000/60000 [==============================] - 0s 4us/sample - loss: 8995.3604 - acc: 0.4105 - val_loss: 5346.4639 - val_acc: 0.5978\n",
      "Epoch 15/100\n",
      "60000/60000 [==============================] - 0s 4us/sample - loss: 5284.5571 - acc: 0.6072 - val_loss: 3263.4072 - val_acc: 0.5502\n",
      "Epoch 16/100\n",
      "60000/60000 [==============================] - 0s 4us/sample - loss: 3205.7849 - acc: 0.5573 - val_loss: 1230.5205 - val_acc: 0.6293\n",
      "Epoch 17/100\n",
      "60000/60000 [==============================] - 0s 4us/sample - loss: 1198.1952 - acc: 0.6410 - val_loss: 3011.6465 - val_acc: 0.5066\n",
      "Epoch 18/100\n",
      "60000/60000 [==============================] - 0s 4us/sample - loss: 2930.6331 - acc: 0.5115 - val_loss: 4545.2119 - val_acc: 0.5887\n",
      "Epoch 19/100\n",
      "60000/60000 [==============================] - 0s 4us/sample - loss: 4492.4097 - acc: 0.5969 - val_loss: 6075.1206 - val_acc: 0.5222\n",
      "Epoch 20/100\n",
      "60000/60000 [==============================] - 0s 4us/sample - loss: 5976.3286 - acc: 0.5271 - val_loss: 5091.0068 - val_acc: 0.5642\n",
      "Epoch 21/100\n",
      "60000/60000 [==============================] - 0s 4us/sample - loss: 5061.5093 - acc: 0.5735 - val_loss: 4197.3159 - val_acc: 0.5874\n",
      "Epoch 22/100\n",
      "60000/60000 [==============================] - 0s 4us/sample - loss: 4149.8652 - acc: 0.5909 - val_loss: 3397.4541 - val_acc: 0.5309\n",
      "Epoch 23/100\n",
      "60000/60000 [==============================] - 0s 4us/sample - loss: 3302.3792 - acc: 0.5388 - val_loss: 2787.7888 - val_acc: 0.6668\n",
      "Epoch 24/100\n",
      "60000/60000 [==============================] - 0s 4us/sample - loss: 2750.5117 - acc: 0.6720 - val_loss: 1986.9072 - val_acc: 0.6730\n",
      "Epoch 25/100\n",
      "60000/60000 [==============================] - 0s 4us/sample - loss: 1940.4946 - acc: 0.6773 - val_loss: 2719.7026 - val_acc: 0.6416\n",
      "Epoch 26/100\n",
      "60000/60000 [==============================] - 0s 4us/sample - loss: 2650.2188 - acc: 0.6443 - val_loss: 3687.3567 - val_acc: 0.5182\n",
      "Epoch 27/100\n",
      "60000/60000 [==============================] - 0s 4us/sample - loss: 3589.8884 - acc: 0.5218 - val_loss: 5192.8481 - val_acc: 0.6067\n",
      "Epoch 28/100\n",
      "60000/60000 [==============================] - 0s 4us/sample - loss: 5146.8345 - acc: 0.6103 - val_loss: 4317.4263 - val_acc: 0.6657\n",
      "Epoch 29/100\n",
      "60000/60000 [==============================] - 0s 4us/sample - loss: 4244.2480 - acc: 0.6723 - val_loss: 2388.7656 - val_acc: 0.6672\n",
      "Epoch 30/100\n",
      "60000/60000 [==============================] - 0s 5us/sample - loss: 2348.9302 - acc: 0.6693 - val_loss: 1832.9692 - val_acc: 0.6977\n",
      "Epoch 31/100\n",
      "60000/60000 [==============================] - 0s 5us/sample - loss: 1772.1372 - acc: 0.7041 - val_loss: 1939.2834 - val_acc: 0.6169\n",
      "Epoch 32/100\n",
      "60000/60000 [==============================] - 0s 5us/sample - loss: 1854.9370 - acc: 0.6292 - val_loss: 2747.8982 - val_acc: 0.6844\n",
      "Epoch 33/100\n",
      "60000/60000 [==============================] - 0s 4us/sample - loss: 2692.6353 - acc: 0.6944 - val_loss: 2559.7344 - val_acc: 0.6860\n",
      "Epoch 34/100\n",
      "60000/60000 [==============================] - 0s 4us/sample - loss: 2495.7593 - acc: 0.6906 - val_loss: 1932.4900 - val_acc: 0.6820\n",
      "Epoch 35/100\n",
      "60000/60000 [==============================] - 0s 4us/sample - loss: 1889.6670 - acc: 0.6855 - val_loss: 2272.1492 - val_acc: 0.6256\n",
      "Epoch 36/100\n",
      "60000/60000 [==============================] - 0s 4us/sample - loss: 2162.8186 - acc: 0.6420 - val_loss: 3059.3320 - val_acc: 0.6451\n",
      "Epoch 37/100\n",
      "60000/60000 [==============================] - 0s 5us/sample - loss: 3003.9136 - acc: 0.6482 - val_loss: 3177.5195 - val_acc: 0.6174\n",
      "Epoch 38/100\n",
      "60000/60000 [==============================] - 0s 5us/sample - loss: 3115.4324 - acc: 0.6262 - val_loss: 2224.2271 - val_acc: 0.6852\n",
      "Epoch 39/100\n",
      "60000/60000 [==============================] - 0s 5us/sample - loss: 2161.4758 - acc: 0.6895 - val_loss: 2555.3311 - val_acc: 0.5668\n",
      "Epoch 40/100\n",
      "60000/60000 [==============================] - 0s 5us/sample - loss: 2446.6914 - acc: 0.5799 - val_loss: 3740.4067 - val_acc: 0.6857\n",
      "Epoch 41/100\n",
      "60000/60000 [==============================] - 0s 5us/sample - loss: 3689.7729 - acc: 0.6930 - val_loss: 3538.2881 - val_acc: 0.6691\n",
      "Epoch 42/100\n",
      "60000/60000 [==============================] - 0s 5us/sample - loss: 3464.5034 - acc: 0.6736 - val_loss: 2562.2937 - val_acc: 0.6744\n",
      "Epoch 43/100\n",
      "60000/60000 [==============================] - 0s 5us/sample - loss: 2515.3918 - acc: 0.6760 - val_loss: 1871.3763 - val_acc: 0.6999\n",
      "Epoch 44/100\n",
      "60000/60000 [==============================] - 0s 5us/sample - loss: 1793.3458 - acc: 0.7123 - val_loss: 1338.9141 - val_acc: 0.7139\n",
      "Epoch 45/100\n",
      "60000/60000 [==============================] - 0s 5us/sample - loss: 1299.8442 - acc: 0.7218 - val_loss: 884.7607 - val_acc: 0.7062\n",
      "Epoch 46/100\n",
      "60000/60000 [==============================] - 0s 5us/sample - loss: 820.7278 - acc: 0.7226 - val_loss: 1656.9977 - val_acc: 0.7194\n",
      "Epoch 47/100\n",
      "60000/60000 [==============================] - 0s 5us/sample - loss: 1623.1593 - acc: 0.7272 - val_loss: 809.5759 - val_acc: 0.7247\n",
      "Epoch 48/100\n",
      "60000/60000 [==============================] - 0s 5us/sample - loss: 761.3002 - acc: 0.7368 - val_loss: 1274.2844 - val_acc: 0.6602\n",
      "Epoch 49/100\n",
      "60000/60000 [==============================] - 0s 5us/sample - loss: 1225.0312 - acc: 0.6655 - val_loss: 2049.7229 - val_acc: 0.6677\n",
      "Epoch 50/100\n",
      "60000/60000 [==============================] - 0s 5us/sample - loss: 1959.1215 - acc: 0.6791 - val_loss: 1202.6426 - val_acc: 0.6796\n",
      "Epoch 51/100\n",
      "60000/60000 [==============================] - 0s 5us/sample - loss: 1159.9763 - acc: 0.6871 - val_loss: 2034.9928 - val_acc: 0.6083\n",
      "Epoch 52/100\n",
      "60000/60000 [==============================] - 0s 5us/sample - loss: 1926.0790 - acc: 0.6202 - val_loss: 2637.3965 - val_acc: 0.7219\n",
      "Epoch 53/100\n",
      "60000/60000 [==============================] - 0s 5us/sample - loss: 2603.9082 - acc: 0.7295 - val_loss: 1522.0276 - val_acc: 0.7191\n",
      "Epoch 54/100\n",
      "60000/60000 [==============================] - 0s 5us/sample - loss: 1480.4426 - acc: 0.7277 - val_loss: 1232.7339 - val_acc: 0.6857\n",
      "Epoch 55/100\n",
      "60000/60000 [==============================] - 0s 5us/sample - loss: 1193.9358 - acc: 0.6873 - val_loss: 1981.7841 - val_acc: 0.6751\n",
      "Epoch 56/100\n",
      "60000/60000 [==============================] - 0s 5us/sample - loss: 1888.8296 - acc: 0.6868 - val_loss: 1225.2412 - val_acc: 0.7243\n",
      "Epoch 57/100\n",
      "60000/60000 [==============================] - 0s 5us/sample - loss: 1188.6632 - acc: 0.7317 - val_loss: 1191.5793 - val_acc: 0.6629\n",
      "Epoch 58/100\n",
      "60000/60000 [==============================] - 0s 5us/sample - loss: 1106.3375 - acc: 0.6779 - val_loss: 2066.3574 - val_acc: 0.7136\n",
      "Epoch 59/100\n",
      "60000/60000 [==============================] - 0s 5us/sample - loss: 2027.7670 - acc: 0.7204 - val_loss: 1786.0792 - val_acc: 0.6827\n",
      "Epoch 60/100\n",
      "60000/60000 [==============================] - 0s 5us/sample - loss: 1723.2819 - acc: 0.6896 - val_loss: 2442.0095 - val_acc: 0.6595\n",
      "Epoch 61/100\n",
      "60000/60000 [==============================] - 0s 5us/sample - loss: 2369.5066 - acc: 0.6767 - val_loss: 2554.5305 - val_acc: 0.7106\n",
      "Epoch 62/100\n",
      "60000/60000 [==============================] - 0s 5us/sample - loss: 2507.1323 - acc: 0.7157 - val_loss: 2528.2463 - val_acc: 0.6807\n",
      "Epoch 63/100\n",
      "60000/60000 [==============================] - 0s 5us/sample - loss: 2460.1809 - acc: 0.6873 - val_loss: 1869.6188 - val_acc: 0.7119\n",
      "Epoch 64/100\n",
      "60000/60000 [==============================] - 0s 5us/sample - loss: 1833.7635 - acc: 0.7219 - val_loss: 973.5504 - val_acc: 0.7368\n",
      "Epoch 65/100\n",
      "60000/60000 [==============================] - 0s 5us/sample - loss: 930.1444 - acc: 0.7473 - val_loss: 1239.3534 - val_acc: 0.6486\n",
      "Epoch 66/100\n",
      "60000/60000 [==============================] - 0s 5us/sample - loss: 1147.9750 - acc: 0.6644 - val_loss: 2526.9475 - val_acc: 0.6959\n",
      "Epoch 67/100\n",
      "60000/60000 [==============================] - 0s 5us/sample - loss: 2490.7014 - acc: 0.7048 - val_loss: 1878.5035 - val_acc: 0.6703\n",
      "Epoch 68/100\n",
      "60000/60000 [==============================] - 0s 5us/sample - loss: 1820.9705 - acc: 0.6751 - val_loss: 2123.0964 - val_acc: 0.6581\n",
      "Epoch 69/100\n",
      "60000/60000 [==============================] - 0s 5us/sample - loss: 2083.5698 - acc: 0.6626 - val_loss: 2787.5479 - val_acc: 0.6131\n",
      "Epoch 70/100\n",
      "60000/60000 [==============================] - 0s 5us/sample - loss: 2667.3306 - acc: 0.6268 - val_loss: 3767.6641 - val_acc: 0.6688\n",
      "Epoch 71/100\n",
      "60000/60000 [==============================] - 0s 5us/sample - loss: 3706.8691 - acc: 0.6753 - val_loss: 3643.5857 - val_acc: 0.6603\n",
      "Epoch 72/100\n",
      "60000/60000 [==============================] - 0s 5us/sample - loss: 3587.6787 - acc: 0.6671 - val_loss: 2000.8024 - val_acc: 0.7083\n",
      "Epoch 73/100\n",
      "60000/60000 [==============================] - 0s 5us/sample - loss: 1936.8910 - acc: 0.7090 - val_loss: 1590.0208 - val_acc: 0.6202\n",
      "Epoch 74/100\n",
      "60000/60000 [==============================] - 0s 5us/sample - loss: 1499.4642 - acc: 0.6348 - val_loss: 2662.0928 - val_acc: 0.7391\n",
      "Epoch 75/100\n",
      "60000/60000 [==============================] - 0s 4us/sample - loss: 2625.0857 - acc: 0.7476 - val_loss: 1868.9208 - val_acc: 0.7087\n",
      "Epoch 76/100\n",
      "60000/60000 [==============================] - 0s 4us/sample - loss: 1813.6790 - acc: 0.7129 - val_loss: 2163.5481 - val_acc: 0.6812\n",
      "Epoch 77/100\n",
      "60000/60000 [==============================] - 0s 4us/sample - loss: 2118.4924 - acc: 0.6854 - val_loss: 3159.2166 - val_acc: 0.6123\n",
      "Epoch 78/100\n",
      "60000/60000 [==============================] - 0s 4us/sample - loss: 3025.8040 - acc: 0.6207 - val_loss: 4051.5415 - val_acc: 0.6798\n",
      "Epoch 79/100\n",
      "60000/60000 [==============================] - 0s 4us/sample - loss: 3983.4858 - acc: 0.6844 - val_loss: 3441.2363 - val_acc: 0.6689\n",
      "Epoch 80/100\n",
      "60000/60000 [==============================] - 0s 4us/sample - loss: 3381.1501 - acc: 0.6746 - val_loss: 1838.0410 - val_acc: 0.6946\n",
      "Epoch 81/100\n",
      "60000/60000 [==============================] - 0s 4us/sample - loss: 1773.5520 - acc: 0.6967 - val_loss: 1526.6392 - val_acc: 0.7120\n",
      "Epoch 82/100\n",
      "60000/60000 [==============================] - 0s 4us/sample - loss: 1463.7949 - acc: 0.7238 - val_loss: 1188.6346 - val_acc: 0.7380\n",
      "Epoch 83/100\n",
      "60000/60000 [==============================] - 0s 4us/sample - loss: 1147.1771 - acc: 0.7462 - val_loss: 840.0160 - val_acc: 0.6965\n",
      "Epoch 84/100\n",
      "60000/60000 [==============================] - 0s 4us/sample - loss: 769.2303 - acc: 0.7182 - val_loss: 1531.1332 - val_acc: 0.7098\n",
      "Epoch 85/100\n",
      "60000/60000 [==============================] - 0s 4us/sample - loss: 1501.8187 - acc: 0.7173 - val_loss: 895.8068 - val_acc: 0.6985\n",
      "Epoch 86/100\n",
      "60000/60000 [==============================] - 0s 4us/sample - loss: 842.5810 - acc: 0.7103 - val_loss: 1056.9060 - val_acc: 0.6944\n",
      "Epoch 87/100\n",
      "60000/60000 [==============================] - 0s 4us/sample - loss: 1010.9723 - acc: 0.6990 - val_loss: 1855.0620 - val_acc: 0.6669\n",
      "Epoch 88/100\n",
      "60000/60000 [==============================] - 0s 4us/sample - loss: 1756.0051 - acc: 0.6796 - val_loss: 1847.6376 - val_acc: 0.7128\n",
      "Epoch 89/100\n",
      "60000/60000 [==============================] - 0s 4us/sample - loss: 1805.1938 - acc: 0.7211 - val_loss: 1331.0094 - val_acc: 0.6328\n",
      "Epoch 90/100\n",
      "60000/60000 [==============================] - 0s 4us/sample - loss: 1263.9181 - acc: 0.6397 - val_loss: 2133.1067 - val_acc: 0.6483\n",
      "Epoch 91/100\n",
      "60000/60000 [==============================] - 0s 4us/sample - loss: 2070.7617 - acc: 0.6562 - val_loss: 1027.3884 - val_acc: 0.7360\n",
      "Epoch 92/100\n",
      "60000/60000 [==============================] - 0s 4us/sample - loss: 977.7676 - acc: 0.7456 - val_loss: 960.6500 - val_acc: 0.7215\n",
      "Epoch 93/100\n",
      "60000/60000 [==============================] - 0s 4us/sample - loss: 887.4335 - acc: 0.7395 - val_loss: 1779.1068 - val_acc: 0.7519\n",
      "Epoch 94/100\n",
      "60000/60000 [==============================] - 0s 4us/sample - loss: 1747.3370 - acc: 0.7635 - val_loss: 799.0787 - val_acc: 0.7696\n",
      "Epoch 95/100\n",
      "60000/60000 [==============================] - 0s 4us/sample - loss: 752.3205 - acc: 0.7768 - val_loss: 1511.6296 - val_acc: 0.6729\n",
      "Epoch 96/100\n",
      "60000/60000 [==============================] - 0s 4us/sample - loss: 1412.1436 - acc: 0.6883 - val_loss: 2541.4956 - val_acc: 0.7604\n",
      "Epoch 97/100\n",
      "60000/60000 [==============================] - 0s 4us/sample - loss: 2511.1152 - acc: 0.7679 - val_loss: 1553.9301 - val_acc: 0.7317\n",
      "Epoch 98/100\n",
      "60000/60000 [==============================] - 0s 4us/sample - loss: 1506.8746 - acc: 0.7375 - val_loss: 1634.6322 - val_acc: 0.6967\n",
      "Epoch 99/100\n",
      "60000/60000 [==============================] - 0s 4us/sample - loss: 1587.4095 - acc: 0.7057 - val_loss: 2605.1045 - val_acc: 0.6576\n",
      "Epoch 100/100\n",
      "60000/60000 [==============================] - 0s 4us/sample - loss: 2480.6313 - acc: 0.6717 - val_loss: 3148.9417 - val_acc: 0.6970\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f6c58d4fa50>"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(trainX, trainY, \n",
    "          validation_data=(testX, testY), \n",
    "          epochs=100,\n",
    "          batch_size=trainX.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JdzDtGwDOIVF"
   },
   "source": [
    "### In the above Neural Network model add Batch Normalization layer after the input layer and repeat the steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kndfpdidOIVI"
   },
   "outputs": [],
   "source": [
    "#Initialize Sequential model\n",
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "#Reshape data from 2D to 1D -> 28x28 to 784\n",
    "model.add(tf.keras.layers.Reshape((784,),input_shape=(28,28,)))\n",
    "\n",
    "#Normalize the data\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "#Add Dense Layer which provides 10 Outputs after applying softmax\n",
    "model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
    "\n",
    "#Comile the model\n",
    "model.compile(optimizer='sgd', loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mwk3T5LJOIVN"
   },
   "source": [
    "### Execute the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 417
    },
    "colab_type": "code",
    "id": "JNLR8tcBOIVP",
    "outputId": "286a0b5b-8f44-4843-f9ab-053188b64554"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 3s 58us/sample - loss: 0.5931 - acc: 0.7969 - val_loss: 0.5342 - val_acc: 0.8194\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 3s 54us/sample - loss: 0.4891 - acc: 0.8303 - val_loss: 0.4932 - val_acc: 0.8308\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 3s 53us/sample - loss: 0.4694 - acc: 0.8381 - val_loss: 0.4778 - val_acc: 0.8313\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 3s 57us/sample - loss: 0.4577 - acc: 0.8415 - val_loss: 0.4753 - val_acc: 0.8352\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 4s 61us/sample - loss: 0.4498 - acc: 0.8449 - val_loss: 0.4735 - val_acc: 0.8349\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 4s 59us/sample - loss: 0.4435 - acc: 0.8470 - val_loss: 0.4645 - val_acc: 0.8362\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 4s 59us/sample - loss: 0.4393 - acc: 0.8493 - val_loss: 0.4647 - val_acc: 0.8378\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 3s 52us/sample - loss: 0.4375 - acc: 0.8482 - val_loss: 0.4601 - val_acc: 0.8399\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 3s 53us/sample - loss: 0.4320 - acc: 0.8503 - val_loss: 0.4635 - val_acc: 0.8391\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 3s 51us/sample - loss: 0.4304 - acc: 0.8500 - val_loss: 0.4575 - val_acc: 0.8399\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f6c59b7d3d0>"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(trainX, trainY, \n",
    "          validation_data=(testX, testY), \n",
    "          epochs=10,\n",
    "          batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Py-KwkmjOIVU"
   },
   "source": [
    "### Customize the learning rate to 0.001 in sgd optimizer and run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 417
    },
    "colab_type": "code",
    "id": "pJUqA5T4OIVc",
    "outputId": "00dc9469-224e-45db-c27b-7cc7ed6b4414"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 0.4164 - acc: 0.8553 - val_loss: 0.4623 - val_acc: 0.8437\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 3s 58us/sample - loss: 0.4161 - acc: 0.8550 - val_loss: 0.4567 - val_acc: 0.8431\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 3s 56us/sample - loss: 0.4148 - acc: 0.8562 - val_loss: 0.4606 - val_acc: 0.8428\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 3s 53us/sample - loss: 0.4141 - acc: 0.8565 - val_loss: 0.4529 - val_acc: 0.8435\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 4s 63us/sample - loss: 0.4155 - acc: 0.8552 - val_loss: 0.4567 - val_acc: 0.8423\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 4s 63us/sample - loss: 0.4162 - acc: 0.8547 - val_loss: 0.4609 - val_acc: 0.8433\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 4s 60us/sample - loss: 0.4142 - acc: 0.8566 - val_loss: 0.4605 - val_acc: 0.8414\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 3s 51us/sample - loss: 0.4137 - acc: 0.8572 - val_loss: 0.4521 - val_acc: 0.8426\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 3s 52us/sample - loss: 0.4133 - acc: 0.8559 - val_loss: 0.4559 - val_acc: 0.8421\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 3s 51us/sample - loss: 0.4138 - acc: 0.8536 - val_loss: 0.4574 - val_acc: 0.8417\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f6c49f30710>"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd_optimizer = tf.keras.optimizers.SGD(learning_rate=0.001)\n",
    "model.compile(optimizer=sgd_optimizer,loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "model.fit(trainX, trainY, \n",
    "          validation_data=(testX, testY), \n",
    "          epochs=10,\n",
    "          batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j9CSqKvpOIVk"
   },
   "source": [
    "### Build the Neural Network model with 3 Dense layers with 100,100,10 neurons respectively in each layer. Use cross entropy loss function and singmoid as activation in the hidden layers and softmax as activation function in the output layer. Use sgd optimizer with learning rate 0.03."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GGAad54JOIVm"
   },
   "outputs": [],
   "source": [
    "sgd_optimizer = tf.keras.optimizers.SGD(learning_rate=0.03)\n",
    "model.add(tf.keras.layers.Dense(100, activation='sigmoid'))\n",
    "model.add(tf.keras.layers.Dense(100, activation='sigmoid'))\n",
    "model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
    "model.compile(optimizer=sgd_optimizer,loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Nr2YsZV0OIV0"
   },
   "source": [
    "## Review model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 381
    },
    "colab_type": "code",
    "id": "h4ojW6-oOIV2",
    "outputId": "9c3fa8ed-4ac6-4548-c4be-05814e0eb1c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "reshape_3 (Reshape)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 784)               3136      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                7850      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 100)               1100      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 23,196\n",
      "Trainable params: 21,628\n",
      "Non-trainable params: 1,568\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gfFGmbZLOIV5"
   },
   "source": [
    "### Run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "bIkbMEN5OIV7",
    "outputId": "4f857b0b-d35d-4fb4-db28-63a12dfda724"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "60000/60000 [==============================] - 4s 75us/sample - loss: 2.2636 - acc: 0.1947 - val_loss: 2.1970 - val_acc: 0.6252\n",
      "Epoch 2/100\n",
      "60000/60000 [==============================] - 4s 63us/sample - loss: 1.9872 - acc: 0.6299 - val_loss: 1.5801 - val_acc: 0.7427\n",
      "Epoch 3/100\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 1.0273 - acc: 0.7482 - val_loss: 0.7186 - val_acc: 0.7569\n",
      "Epoch 4/100\n",
      "60000/60000 [==============================] - 4s 62us/sample - loss: 0.6339 - acc: 0.7761 - val_loss: 0.5944 - val_acc: 0.7758\n",
      "Epoch 5/100\n",
      "60000/60000 [==============================] - 4s 59us/sample - loss: 0.5705 - acc: 0.7847 - val_loss: 0.5797 - val_acc: 0.7702\n",
      "Epoch 6/100\n",
      "60000/60000 [==============================] - 4s 59us/sample - loss: 0.5555 - acc: 0.7940 - val_loss: 0.5596 - val_acc: 0.8083\n",
      "Epoch 7/100\n",
      "60000/60000 [==============================] - 4s 59us/sample - loss: 0.5229 - acc: 0.8376 - val_loss: 0.5188 - val_acc: 0.8412\n",
      "Epoch 8/100\n",
      "60000/60000 [==============================] - 4s 61us/sample - loss: 0.4946 - acc: 0.8466 - val_loss: 0.5051 - val_acc: 0.8416\n",
      "Epoch 9/100\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 0.4808 - acc: 0.8510 - val_loss: 0.5119 - val_acc: 0.8389\n",
      "Epoch 10/100\n",
      "60000/60000 [==============================] - 4s 59us/sample - loss: 0.4728 - acc: 0.8517 - val_loss: 0.4928 - val_acc: 0.8444\n",
      "Epoch 11/100\n",
      "60000/60000 [==============================] - 4s 60us/sample - loss: 0.4715 - acc: 0.8538 - val_loss: 0.5208 - val_acc: 0.8357\n",
      "Epoch 12/100\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.4648 - acc: 0.8541 - val_loss: 0.5039 - val_acc: 0.8404\n",
      "Epoch 13/100\n",
      "60000/60000 [==============================] - 4s 71us/sample - loss: 0.4664 - acc: 0.8529 - val_loss: 0.4993 - val_acc: 0.8416\n",
      "Epoch 14/100\n",
      "60000/60000 [==============================] - 4s 70us/sample - loss: 0.4629 - acc: 0.8544 - val_loss: 0.5084 - val_acc: 0.8397\n",
      "Epoch 15/100\n",
      "60000/60000 [==============================] - 4s 59us/sample - loss: 0.4647 - acc: 0.8524 - val_loss: 0.5230 - val_acc: 0.8366\n",
      "Epoch 16/100\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.4571 - acc: 0.8559 - val_loss: 0.5064 - val_acc: 0.8427\n",
      "Epoch 17/100\n",
      "60000/60000 [==============================] - 4s 70us/sample - loss: 0.4549 - acc: 0.8550 - val_loss: 0.5046 - val_acc: 0.8405\n",
      "Epoch 18/100\n",
      "60000/60000 [==============================] - 4s 70us/sample - loss: 0.4567 - acc: 0.8548 - val_loss: 0.5046 - val_acc: 0.8404\n",
      "Epoch 19/100\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 0.4507 - acc: 0.8578 - val_loss: 0.5083 - val_acc: 0.8370\n",
      "Epoch 20/100\n",
      "60000/60000 [==============================] - 4s 62us/sample - loss: 0.4478 - acc: 0.8587 - val_loss: 0.5032 - val_acc: 0.8422\n",
      "Epoch 21/100\n",
      "60000/60000 [==============================] - 4s 64us/sample - loss: 0.4506 - acc: 0.8576 - val_loss: 0.5130 - val_acc: 0.8422\n",
      "Epoch 22/100\n",
      "60000/60000 [==============================] - 4s 72us/sample - loss: 0.4496 - acc: 0.8582 - val_loss: 0.4987 - val_acc: 0.8457\n",
      "Epoch 23/100\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.4479 - acc: 0.8596 - val_loss: 0.4961 - val_acc: 0.8446\n",
      "Epoch 24/100\n",
      "60000/60000 [==============================] - 4s 64us/sample - loss: 0.4470 - acc: 0.8582 - val_loss: 0.5082 - val_acc: 0.8394\n",
      "Epoch 25/100\n",
      "60000/60000 [==============================] - 4s 61us/sample - loss: 0.4479 - acc: 0.8577 - val_loss: 0.4988 - val_acc: 0.8471\n",
      "Epoch 26/100\n",
      "60000/60000 [==============================] - 4s 61us/sample - loss: 0.4495 - acc: 0.8573 - val_loss: 0.5074 - val_acc: 0.8398\n",
      "Epoch 27/100\n",
      "60000/60000 [==============================] - 4s 60us/sample - loss: 0.4451 - acc: 0.8604 - val_loss: 0.5005 - val_acc: 0.8430\n",
      "Epoch 28/100\n",
      "60000/60000 [==============================] - 4s 62us/sample - loss: 0.4446 - acc: 0.8578 - val_loss: 0.5193 - val_acc: 0.8390\n",
      "Epoch 29/100\n",
      "60000/60000 [==============================] - 3s 58us/sample - loss: 0.4437 - acc: 0.8589 - val_loss: 0.5065 - val_acc: 0.8415\n",
      "Epoch 30/100\n",
      "60000/60000 [==============================] - 4s 61us/sample - loss: 0.4420 - acc: 0.8594 - val_loss: 0.4958 - val_acc: 0.8416\n",
      "Epoch 31/100\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 0.4450 - acc: 0.8583 - val_loss: 0.5090 - val_acc: 0.8390\n",
      "Epoch 32/100\n",
      "60000/60000 [==============================] - 4s 61us/sample - loss: 0.4436 - acc: 0.8594 - val_loss: 0.4918 - val_acc: 0.8476\n",
      "Epoch 33/100\n",
      "60000/60000 [==============================] - 4s 59us/sample - loss: 0.4427 - acc: 0.8581 - val_loss: 0.4959 - val_acc: 0.8430\n",
      "Epoch 34/100\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 0.4393 - acc: 0.8605 - val_loss: 0.5056 - val_acc: 0.8432\n",
      "Epoch 35/100\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.4398 - acc: 0.8600 - val_loss: 0.5045 - val_acc: 0.8416\n",
      "Epoch 36/100\n",
      "60000/60000 [==============================] - 4s 73us/sample - loss: 0.4351 - acc: 0.8612 - val_loss: 0.5099 - val_acc: 0.8410\n",
      "Epoch 37/100\n",
      "60000/60000 [==============================] - 4s 63us/sample - loss: 0.4378 - acc: 0.8606 - val_loss: 0.4889 - val_acc: 0.8461\n",
      "Epoch 38/100\n",
      "60000/60000 [==============================] - 4s 64us/sample - loss: 0.4386 - acc: 0.8601 - val_loss: 0.5016 - val_acc: 0.8445\n",
      "Epoch 39/100\n",
      "60000/60000 [==============================] - 4s 66us/sample - loss: 0.4369 - acc: 0.8620 - val_loss: 0.5166 - val_acc: 0.8354\n",
      "Epoch 40/100\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 0.4361 - acc: 0.8595 - val_loss: 0.5135 - val_acc: 0.8372\n",
      "Epoch 41/100\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 0.4370 - acc: 0.8605 - val_loss: 0.5136 - val_acc: 0.8394\n",
      "Epoch 42/100\n",
      "60000/60000 [==============================] - 3s 58us/sample - loss: 0.4364 - acc: 0.8609 - val_loss: 0.4936 - val_acc: 0.8445\n",
      "Epoch 43/100\n",
      "60000/60000 [==============================] - 4s 59us/sample - loss: 0.4356 - acc: 0.8607 - val_loss: 0.5086 - val_acc: 0.8414\n",
      "Epoch 44/100\n",
      "60000/60000 [==============================] - 4s 73us/sample - loss: 0.4341 - acc: 0.8610 - val_loss: 0.5049 - val_acc: 0.8416\n",
      "Epoch 45/100\n",
      "60000/60000 [==============================] - 4s 74us/sample - loss: 0.4351 - acc: 0.8609 - val_loss: 0.5031 - val_acc: 0.8430\n",
      "Epoch 46/100\n",
      "60000/60000 [==============================] - 4s 60us/sample - loss: 0.4307 - acc: 0.8628 - val_loss: 0.4920 - val_acc: 0.8463\n",
      "Epoch 47/100\n",
      "60000/60000 [==============================] - 4s 59us/sample - loss: 0.4310 - acc: 0.8628 - val_loss: 0.4904 - val_acc: 0.8445\n",
      "Epoch 48/100\n",
      "60000/60000 [==============================] - 4s 59us/sample - loss: 0.4356 - acc: 0.8606 - val_loss: 0.4961 - val_acc: 0.8447\n",
      "Epoch 49/100\n",
      "60000/60000 [==============================] - 4s 61us/sample - loss: 0.4338 - acc: 0.8611 - val_loss: 0.4928 - val_acc: 0.8463\n",
      "Epoch 50/100\n",
      "60000/60000 [==============================] - 4s 60us/sample - loss: 0.4323 - acc: 0.8626 - val_loss: 0.5104 - val_acc: 0.8387\n",
      "Epoch 51/100\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.4310 - acc: 0.8611 - val_loss: 0.4941 - val_acc: 0.8448\n",
      "Epoch 52/100\n",
      "60000/60000 [==============================] - 4s 62us/sample - loss: 0.4289 - acc: 0.8628 - val_loss: 0.4954 - val_acc: 0.8435\n",
      "Epoch 53/100\n",
      "60000/60000 [==============================] - 4s 62us/sample - loss: 0.4301 - acc: 0.8629 - val_loss: 0.4922 - val_acc: 0.8427\n",
      "Epoch 54/100\n",
      "60000/60000 [==============================] - 4s 60us/sample - loss: 0.4289 - acc: 0.8632 - val_loss: 0.5038 - val_acc: 0.8440\n",
      "Epoch 55/100\n",
      "60000/60000 [==============================] - 4s 61us/sample - loss: 0.4276 - acc: 0.8631 - val_loss: 0.4958 - val_acc: 0.8425\n",
      "Epoch 56/100\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 0.4303 - acc: 0.8620 - val_loss: 0.4919 - val_acc: 0.8455\n",
      "Epoch 57/100\n",
      "60000/60000 [==============================] - 4s 61us/sample - loss: 0.4277 - acc: 0.8621 - val_loss: 0.4984 - val_acc: 0.8436\n",
      "Epoch 58/100\n",
      "60000/60000 [==============================] - 4s 60us/sample - loss: 0.4245 - acc: 0.8633 - val_loss: 0.4903 - val_acc: 0.8447\n",
      "Epoch 59/100\n",
      "60000/60000 [==============================] - 4s 59us/sample - loss: 0.4264 - acc: 0.8630 - val_loss: 0.4960 - val_acc: 0.8442\n",
      "Epoch 60/100\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.4270 - acc: 0.8628 - val_loss: 0.4944 - val_acc: 0.8452\n",
      "Epoch 61/100\n",
      "60000/60000 [==============================] - 4s 74us/sample - loss: 0.4290 - acc: 0.8623 - val_loss: 0.5041 - val_acc: 0.8399\n",
      "Epoch 62/100\n",
      "60000/60000 [==============================] - 4s 74us/sample - loss: 0.4257 - acc: 0.8643 - val_loss: 0.5102 - val_acc: 0.8381\n",
      "Epoch 63/100\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.4279 - acc: 0.8624 - val_loss: 0.4987 - val_acc: 0.8427\n",
      "Epoch 64/100\n",
      "60000/60000 [==============================] - 4s 64us/sample - loss: 0.4259 - acc: 0.8632 - val_loss: 0.4996 - val_acc: 0.8413\n",
      "Epoch 65/100\n",
      "60000/60000 [==============================] - 4s 72us/sample - loss: 0.4251 - acc: 0.8639 - val_loss: 0.5056 - val_acc: 0.8407\n",
      "Epoch 66/100\n",
      "60000/60000 [==============================] - 4s 63us/sample - loss: 0.4238 - acc: 0.8630 - val_loss: 0.4920 - val_acc: 0.8452\n",
      "Epoch 67/100\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 0.4249 - acc: 0.8634 - val_loss: 0.4931 - val_acc: 0.8412\n",
      "Epoch 68/100\n",
      "60000/60000 [==============================] - 4s 64us/sample - loss: 0.4213 - acc: 0.8635 - val_loss: 0.4936 - val_acc: 0.8439\n",
      "Epoch 69/100\n",
      "60000/60000 [==============================] - 3s 58us/sample - loss: 0.4230 - acc: 0.8644 - val_loss: 0.4945 - val_acc: 0.8433\n",
      "Epoch 70/100\n",
      "60000/60000 [==============================] - 4s 73us/sample - loss: 0.4252 - acc: 0.8636 - val_loss: 0.5107 - val_acc: 0.8358\n",
      "Epoch 71/100\n",
      "60000/60000 [==============================] - 4s 66us/sample - loss: 0.4232 - acc: 0.8636 - val_loss: 0.4926 - val_acc: 0.8466\n",
      "Epoch 72/100\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 0.4212 - acc: 0.8643 - val_loss: 0.5117 - val_acc: 0.8382\n",
      "Epoch 73/100\n",
      "60000/60000 [==============================] - 4s 66us/sample - loss: 0.4205 - acc: 0.8632 - val_loss: 0.4972 - val_acc: 0.8424\n",
      "Epoch 74/100\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 0.4193 - acc: 0.8640 - val_loss: 0.4991 - val_acc: 0.8398\n",
      "Epoch 75/100\n",
      "60000/60000 [==============================] - 4s 64us/sample - loss: 0.4215 - acc: 0.8636 - val_loss: 0.4882 - val_acc: 0.8470\n",
      "Epoch 76/100\n",
      "60000/60000 [==============================] - 4s 64us/sample - loss: 0.4234 - acc: 0.8634 - val_loss: 0.4913 - val_acc: 0.8423\n",
      "Epoch 77/100\n",
      "60000/60000 [==============================] - 4s 75us/sample - loss: 0.4209 - acc: 0.8637 - val_loss: 0.4972 - val_acc: 0.8400\n",
      "Epoch 78/100\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 0.4196 - acc: 0.8648 - val_loss: 0.4907 - val_acc: 0.8434\n",
      "Epoch 79/100\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 0.4230 - acc: 0.8626 - val_loss: 0.4903 - val_acc: 0.8446\n",
      "Epoch 80/100\n",
      "60000/60000 [==============================] - 4s 70us/sample - loss: 0.4213 - acc: 0.8645 - val_loss: 0.4919 - val_acc: 0.8454\n",
      "Epoch 81/100\n",
      "60000/60000 [==============================] - 4s 62us/sample - loss: 0.4185 - acc: 0.8656 - val_loss: 0.5008 - val_acc: 0.8406\n",
      "Epoch 82/100\n",
      "60000/60000 [==============================] - 5s 77us/sample - loss: 0.4175 - acc: 0.8643 - val_loss: 0.4885 - val_acc: 0.8441\n",
      "Epoch 83/100\n",
      "60000/60000 [==============================] - 5s 77us/sample - loss: 0.4199 - acc: 0.8629 - val_loss: 0.4954 - val_acc: 0.8416\n",
      "Epoch 84/100\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 0.4144 - acc: 0.8661 - val_loss: 0.4901 - val_acc: 0.8450\n",
      "Epoch 85/100\n",
      "60000/60000 [==============================] - 4s 61us/sample - loss: 0.4172 - acc: 0.8649 - val_loss: 0.4808 - val_acc: 0.8492\n",
      "Epoch 86/100\n",
      "60000/60000 [==============================] - 4s 66us/sample - loss: 0.4159 - acc: 0.8664 - val_loss: 0.4858 - val_acc: 0.8417\n",
      "Epoch 87/100\n",
      "60000/60000 [==============================] - 4s 73us/sample - loss: 0.4193 - acc: 0.8635 - val_loss: 0.4841 - val_acc: 0.8443\n",
      "Epoch 88/100\n",
      "60000/60000 [==============================] - 4s 62us/sample - loss: 0.4193 - acc: 0.8627 - val_loss: 0.4997 - val_acc: 0.8411\n",
      "Epoch 89/100\n",
      "60000/60000 [==============================] - 4s 64us/sample - loss: 0.4163 - acc: 0.8645 - val_loss: 0.5037 - val_acc: 0.8404\n",
      "Epoch 90/100\n",
      "60000/60000 [==============================] - 4s 74us/sample - loss: 0.4170 - acc: 0.8636 - val_loss: 0.4906 - val_acc: 0.8411\n",
      "Epoch 91/100\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 0.4149 - acc: 0.8656 - val_loss: 0.4851 - val_acc: 0.8462\n",
      "Epoch 92/100\n",
      "60000/60000 [==============================] - 4s 71us/sample - loss: 0.4146 - acc: 0.8650 - val_loss: 0.5066 - val_acc: 0.8389\n",
      "Epoch 93/100\n",
      "60000/60000 [==============================] - 4s 74us/sample - loss: 0.4163 - acc: 0.8645 - val_loss: 0.4879 - val_acc: 0.8452\n",
      "Epoch 94/100\n",
      "60000/60000 [==============================] - 5s 75us/sample - loss: 0.4142 - acc: 0.8634 - val_loss: 0.4965 - val_acc: 0.8399\n",
      "Epoch 95/100\n",
      "60000/60000 [==============================] - 4s 59us/sample - loss: 0.4161 - acc: 0.8631 - val_loss: 0.4945 - val_acc: 0.8405\n",
      "Epoch 96/100\n",
      "60000/60000 [==============================] - 4s 62us/sample - loss: 0.4182 - acc: 0.8628 - val_loss: 0.4931 - val_acc: 0.8411\n",
      "Epoch 97/100\n",
      "60000/60000 [==============================] - 4s 60us/sample - loss: 0.4158 - acc: 0.8642 - val_loss: 0.4912 - val_acc: 0.8409\n",
      "Epoch 98/100\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 0.4155 - acc: 0.8641 - val_loss: 0.4967 - val_acc: 0.8391\n",
      "Epoch 99/100\n",
      "60000/60000 [==============================] - 4s 63us/sample - loss: 0.4163 - acc: 0.8627 - val_loss: 0.4892 - val_acc: 0.8394\n",
      "Epoch 100/100\n",
      "60000/60000 [==============================] - 3s 58us/sample - loss: 0.4127 - acc: 0.8649 - val_loss: 0.4892 - val_acc: 0.8426\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f6c47dca4d0>"
      ]
     },
     "execution_count": 29,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(trainX,trainY,          \n",
    "          validation_data=(testX,testY),\n",
    "          epochs=100,\n",
    "          batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KtiY1NnA0Xyd"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "R6_ExternalLab_Delhi.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
